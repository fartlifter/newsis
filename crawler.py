import streamlit as st
import requests
from bs4 import BeautifulSoup
from datetime import datetime, time
from zoneinfo import ZoneInfo
import time as t
import re
from concurrent.futures import ThreadPoolExecutor, as_completed

# === ì¸ì¦ ì •ë³´ ===
client_id = "R7Q2OeVNhj8wZtNNFBwL"
client_secret = "49E810CBKY"

# === ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ===
for key in ["articles", "status_text", "progress"]:
    if key not in st.session_state:
        st.session_state[key] = [] if key == "articles" else 0 if key == "progress" else ""

# === ê³µí†µ í•¨ìˆ˜ ===
def parse_pubdate(pubdate_str):
    try:
        return datetime.strptime(pubdate_str, "%a, %d %b %Y %H:%M:%S %z")
    except:
        return None

def extract_title_and_body(url):
    try:
        if "n.news.naver.com" not in url:
            return None, None
        html = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=5)
        if html.status_code != 200:
            return None, None
        soup = BeautifulSoup(html.text, "html.parser")
        title_div = soup.find("div", class_="media_end_head_title")
        content_div = soup.find("div", id="newsct_article")
        title = title_div.get_text(strip=True) if title_div else None
        body = content_div.get_text(separator="\n", strip=True) if content_div else None
        return title, body
    except:
        return None, None

def extract_media_name(url):
    try:
        domain = url.split("//")[-1].split("/")[0]
        parts = domain.split(".")
        if len(parts) >= 3:
            composite_key = f"{parts[-3]}.{parts[-2]}"
        else:
            composite_key = parts[0]
        media_mapping = {
            "chosun": "ì¡°ì„ ", "joongang": "ì¤‘ì•™", "donga": "ë™ì•„", "hani": "í•œê²¨ë ˆ",
            "khan": "ê²½í–¥", "hankookilbo": "í•œêµ­", "segye": "ì„¸ê³„", "seoul": "ì„œìš¸",
            "kmib": "êµ­ë¯¼", "munhwa": "ë¬¸í™”", "kbs": "KBS", "sbs": "SBS",
            "imnews": "MBC", "jtbc": "JTBC", "ichannela": "ì±„ë„A", "tvchosun": "TVì¡°ì„ ",
            "mk": "ë§¤ê²½", "sedaily": "ì„œê²½", "hankyung": "í•œê²½", "news1": "ë‰´ìŠ¤1",
            "newsis": "ë‰´ì‹œìŠ¤", "yna": "ì—°í•©", "mt": "ë¨¸íˆ¬", "weekly": "ì£¼ê°„ì¡°ì„ ",
            "biz.chosun": "ì¡°ì„ ë¹„ì¦ˆ", "fnnews": "íŒŒë‰´"
        }
        if composite_key in media_mapping:
            return media_mapping[composite_key]
        for part in reversed(parts):
            if part in media_mapping:
                return media_mapping[part]
        return composite_key.upper()
    except:
        return "[ë§¤ì²´ì¶”ì¶œì‹¤íŒ¨]"

def safe_api_request(url, headers, params, max_retries=3):
    for _ in range(max_retries):
        try:
            res = requests.get(url, headers=headers, params=params, timeout=5)
            if res.status_code == 200:
                return res
            t.sleep(0.5)
        except:
            t.sleep(0.5)
    return res

def fetch_and_filter(item_data):
    item, start_dt, end_dt, selected_keywords, use_keyword_filter = item_data
    link = item.get("link")
    if not link or "n.news.naver.com" not in link:
        return None

    title, body = extract_title_and_body(link)
    if not title or "[ë‹¨ë…]" not in title or not body:
        return None

    pub_dt = parse_pubdate(item.get("pubDate"))
    if not pub_dt or not (start_dt <= pub_dt <= end_dt):
        return None

    matched_keywords = []
    if use_keyword_filter and selected_keywords:
        matched_keywords = [kw for kw in selected_keywords if kw in body]
        if not matched_keywords:
            return None

    highlighted_body = body
    for kw in matched_keywords:
        highlighted_body = highlighted_body.replace(kw, f"<mark>{kw}</mark>")
    highlighted_body = highlighted_body.replace("\n", "<br><br>")
    media = extract_media_name(item.get("originallink", ""))

    return {
        "í‚¤ì›Œë“œ": "[ë‹¨ë…]",
        "ë§¤ì²´": media,
        "ì œëª©": title,
        "ë‚ ì§œ": pub_dt.strftime("%Y-%m-%d %H:%M:%S"),
        "ë³¸ë¬¸": body,
        "í•„í„°ì¼ì¹˜": ", ".join(matched_keywords),
        "ë§í¬": link,
        "í•˜ì´ë¼ì´íŠ¸": highlighted_body,
        "pub_dt": pub_dt
    }

# === í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ ===
keyword_groups = {
    'ì‹œê²½': ['ì„œìš¸ê²½ì°°ì²­'],
    'ë³¸ì²­': ['ê²½ì°°ì²­'],
    'ì¢…í˜œë¶': ['ì¢…ë¡œ', 'ì¢…ì•”', 'ì„±ë¶', 'ê³ ë ¤ëŒ€', 'ì°¸ì—¬ì—°ëŒ€', 'í˜œí™”', 'ë™ëŒ€ë¬¸', 'ì¤‘ë‘',
        'ì„±ê· ê´€ëŒ€', 'í•œêµ­ì™¸ëŒ€', 'ì„œìš¸ì‹œë¦½ëŒ€', 'ê²½í¬ëŒ€', 'ê²½ì‹¤ë ¨', 'ì„œìš¸ëŒ€ë³‘ì›',
        'ë…¸ì›', 'ê°•ë¶', 'ë„ë´‰', 'ë¶ë¶€ì§€ë²•', 'ë¶ë¶€ì§€ê²€', 'ìƒê³„ë°±ë³‘ì›', 'êµ­ê°€ì¸ê¶Œìœ„ì›íšŒ'],
    'ë§ˆí¬ì¤‘ë¶€': ['ë§ˆí¬', 'ì„œëŒ€ë¬¸', 'ì„œë¶€', 'ì€í‰', 'ì„œë¶€ì§€ê²€', 'ì„œë¶€ì§€ë²•', 'ì—°ì„¸ëŒ€',
        'ì‹ ì´Œì„¸ë¸Œë€ìŠ¤ë³‘ì›', 'êµ°ì¸ê¶Œì„¼í„°', 'ì¤‘ë¶€', 'ë‚¨ëŒ€ë¬¸', 'ìš©ì‚°', 'ë™êµ­ëŒ€', 'ìˆ™ëª…ì—¬ëŒ€', 'ìˆœì²œí–¥ëŒ€ë³‘ì›'],
    'ì˜ë“±í¬ê´€ì•…': ['ì˜ë“±í¬', 'ì–‘ì²œ', 'êµ¬ë¡œ', 'ê°•ì„œ', 'ë‚¨ë¶€ì§€ê²€', 'ë‚¨ë¶€ì§€ë²•', 'ì—¬ì˜ë„ì„±ëª¨ë³‘ì›',
        'ê³ ëŒ€êµ¬ë¡œë³‘ì›', 'ê´€ì•…', 'ê¸ˆì²œ', 'ë™ì‘', 'ë°©ë°°', 'ì„œìš¸ëŒ€', 'ì¤‘ì•™ëŒ€', 'ìˆ­ì‹¤ëŒ€', 'ë³´ë¼ë§¤ë³‘ì›'],
    'ê°•ë‚¨ê´‘ì§„': ['ê°•ë‚¨', 'ì„œì´ˆ', 'ìˆ˜ì„œ', 'ì†¡íŒŒ', 'ê°•ë™', 'ì‚¼ì„±ì˜ë£Œì›', 'í˜„ëŒ€ì•„ì‚°ë³‘ì›',
        'ê°•ë‚¨ì„¸ë¸Œë€ìŠ¤ë³‘ì›', 'ê´‘ì§„', 'ì„±ë™', 'ë™ë¶€ì§€ê²€', 'ë™ë¶€ì§€ë²•', 'í•œì–‘ëŒ€', 'ê±´êµ­ëŒ€', 'ì„¸ì¢…ëŒ€']
}

# === Streamlit UI ===
st.title("ğŸ“° ë‹¨ë…ê¸°ì‚¬ ìˆ˜ì§‘ê¸°_ê²½ì°°íŒ€")
st.markdown("âœ… [ë‹¨ë…] ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ê³  ì„ íƒí•œ í‚¤ì›Œë“œê°€ ë³¸ë¬¸ì— í¬í•¨ëœ ê¸°ì‚¬ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤. ì„ íƒí•œ ê¸°ì‚¬ë§Œ ìµœí•˜ë‹¨ ë³µì‚¬ìš© ë°•ìŠ¤ì— í‘œì‹œë©ë‹ˆë‹¤. ì—…ë°ì´íŠ¸:250622 1815")

now = datetime.now(ZoneInfo("Asia/Seoul"))
today = now.date()

col1, col2 = st.columns(2)
with col1:
    start_date = st.date_input("ì‹œì‘ ë‚ ì§œ", value=today)
    start_time = st.time_input("ì‹œì‘ ì‹œê°", value=time(0, 0))
    start_dt = datetime.combine(start_date, start_time).replace(tzinfo=ZoneInfo("Asia/Seoul"))

with col2:
    end_date = st.date_input("ì¢…ë£Œ ë‚ ì§œ", value=today, key="end_date")
    end_time = st.time_input("ì¢…ë£Œ ì‹œê°", value=time(now.hour, now.minute))
    end_dt = datetime.combine(end_date, end_time).replace(tzinfo=ZoneInfo("Asia/Seoul"))

group_labels = list(keyword_groups.keys())
default_groups = ['ì‹œê²½', 'ì¢…í˜œë¶']
selected_groups = st.multiselect("ğŸ“š ì§€ì—­ ê·¸ë£¹ ì„ íƒ", group_labels, default=default_groups)

selected_keywords = []
for group in selected_groups:
    selected_keywords.extend(keyword_groups[group])

use_keyword_filter = st.checkbox("ğŸ“ í‚¤ì›Œë“œ í¬í•¨ ê¸°ì‚¬ë§Œ í•„í„°ë§", value=True)

# ì§„í–‰ ìƒíƒœ í‘œì‹œ
status_placeholder = st.empty()
progress_bar = st.progress(st.session_state["progress"])
status_placeholder.markdown(st.session_state["status_text"])

# === ìˆ˜ì§‘ ë²„íŠ¼ ===
if st.button("âœ… [ë‹¨ë…] ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘"):
    with st.spinner("ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘..."):
        headers = {
            "X-Naver-Client-Id": client_id,
            "X-Naver-Client-Secret": client_secret
        }
        seen_links = set()
        all_articles = []
        total = 0

        for start_index in range(1, 1001, 100):
            progress = start_index / 1000
            st.session_state["progress"] = progress
            progress_bar.progress(progress)
            st.session_state["status_text"] = f"ğŸŸ¡ ìˆ˜ì§‘ ì¤‘... {total}ê±´ ìˆ˜ì§‘ë¨"
            status_placeholder.markdown(st.session_state["status_text"])

            params = {
                "query": "[ë‹¨ë…]",
                "sort": "date",
                "display": 100,
                "start": start_index
            }
            res = safe_api_request("https://openapi.naver.com/v1/search/news.json", headers, params)
            if res.status_code != 200:
                st.warning(f"API í˜¸ì¶œ ì‹¤íŒ¨: {res.status_code}")
                break
            items = res.json().get("items", [])
            if not items:
                break

            with ThreadPoolExecutor(max_workers=25) as executor:
                futures = [
                    executor.submit(fetch_and_filter, (item, start_dt, end_dt, selected_keywords, use_keyword_filter))
                    for item in items
                ]
                for future in as_completed(futures):
                    result = future.result()
                    if result and result["ë§í¬"] not in seen_links:
                        seen_links.add(result["ë§í¬"])
                        all_articles.append(result)
                        total += 1

        st.session_state["articles"] = all_articles
        st.session_state["status_text"] = f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_articles)}ê±´"
        st.session_state["progress"] = 1.0
        status_placeholder.markdown(st.session_state["status_text"])
        progress_bar.progress(1.0)

# === ê¸°ì‚¬ í‘œì‹œ ë° ì²´í¬ë°•ìŠ¤ ===
selected_articles = []
for idx, result in enumerate(st.session_state["articles"]):
    is_selected = st.checkbox(f"â–³{result['ë§¤ì²´']} / {result['ì œëª©']}", key=f"chk_{idx}")
    st.caption(result["ë‚ ì§œ"])
    if result["í•„í„°ì¼ì¹˜"]:
        st.write(f"**ì¼ì¹˜ í‚¤ì›Œë“œ:** {result['í•„í„°ì¼ì¹˜']}")
    st.markdown(f"- {result['í•˜ì´ë¼ì´íŠ¸']}", unsafe_allow_html=True)
    if is_selected:
        selected_articles.append(result)

# === ë³µì‚¬ ë°•ìŠ¤ ===
if selected_articles:
    text_block = ""
    for row in selected_articles:
        clean_title = re.sub(r"\[ë‹¨ë…\]|\(ë‹¨ë…\)|ã€ë‹¨ë…ã€‘|â“§ë‹¨ë…|^ë‹¨ë…\s*[:-]?", "", row['ì œëª©']).strip()
        text_block += f"â–³{row['ë§¤ì²´']} / {clean_title}\n- {row['ë³¸ë¬¸']}\n\n"
    st.code(text_block.strip(), language="markdown")
    st.caption("âœ… ë³µì‚¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ ì„ íƒí•œ ê¸°ì‚¬ ë‚´ìš©ì„ ë³µì‚¬í•˜ì„¸ìš”.")
